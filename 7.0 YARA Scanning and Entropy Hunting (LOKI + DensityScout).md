## 7.1 Explain what YARA rules detect and why rules can be brittle or noisy

- What YARA is doing
  - Pattern matching over file content and metadata.
  - Typically matches strings, byte sequences, PE characteristics, or structural traits.
  - Best used to quickly identify known families, tooling, or suspicious traits at scale.
- What YARA is NOT doing
  - It does not "understand" behavior.
  - It does not prove execution or compromise by itself.
- Why YARA can be brittle
  - Small changes to malware can break signatures (packing, recompile, string changes).
  - Rules may be written for one build or one campaign and not generalize well.
- Why YARA can be noisy
  - Shared libraries and common code reuse create accidental matches.
  - Generic strings (e.g., common API names, packer markers) increase false positives.
  - Low-quality rules overmatch and create alert fatigue.

## 7.2 Execute LOKI scans against mounted evidence and interpret results safely

- Pre-scan setup
  - Ensure evidence is mounted read-only.
  - Decide scope: full volume scan vs targeted directories.
  - Plan where output will be saved (`logs/` plus a separate results file).
- Execution principles
  - Run scans against mounted evidence paths, not against your tool directories.
  - Prefer targeted scanning when time is limited (user-writable paths first).
  - Keep the VM offline during scanning unless policy requires otherwise.
- Safe interpretation
  - Treat a hit as a lead, not a conclusion.
  - Capture full context for each hit:
    - exact file path
    - hash (SHA256)
    - file size and timestamps
    - rule name and matched strings/offsets (when available)
  - Validate the hit by reviewing the file outside of execution:
    - static analysis, metadata review, and correlation with persistence/execution artifacts

## 7.3 Identify sources of YARA false positives (shared code, packers, generic strings, rule quality)

- Shared code and libraries
  - Commodity malware and legit software can share open-source components.
  - Admin tools may share snippets that overlap with rule strings.
- Packers and protectors
  - Packed binaries often contain recognizable packer stubs.
  - Many legit installers and commercial apps are packed or compressed.
- Generic or overly broad strings
  - Rules that match common words, common API names, or short byte patterns.
  - Rules lacking filesize constraints, module checks, or context conditions.
- Rule quality issues
  - Poorly tested rules that were not validated against large benign corpora.
  - Rules written for one environment that do not translate to yours.
- How to reduce false positives operationally
  - Prefer curated rule sets with known provenance.
  - Require corroboration from local artifacts before elevating severity.
  - Maintain an allowlist of known benign hits in your environment (with evidence).

## 7.4 Use DensityScout to identify high-entropy targets and prioritize review

- What entropy hunting is doing
  - It flags files or regions that look compressed or encrypted.
  - High entropy often indicates packing, encryption, or embedded payloads.
- What high entropy means in practice
  - Many malicious binaries are packed, so entropy is a good lead generator.
  - Many legitimate files are also high entropy (installers, archives, media, browsers).
- How to prioritize high-entropy findings
  - Prioritize by location and context:
    - user-writable directories
    - startup/persistence-associated paths
    - recently created or modified files near the incident window
  - Prioritize by file type:
    - executables, DLLs, scripts with embedded blobs, suspicious archives
  - Combine with basic triage:
    - hashes, signatures, and metadata checks before deeper work

## 7.5 Correlate scanner findings with disk context (location, persistence ties, execution evidence, surrounding artifacts)

- Location-based correlation (fast, high-signal)
  - User-writable paths (`Downloads`, `AppData`, temp directories) raise suspicion.
  - System directories raise suspicion only when publisher/path/version do not line up.
- Persistence linkage (turns a hit into evidence)
  - Look for references to the file in:
    - services, scheduled tasks, Run keys, startup folders, WMI subscriptions
  - A scanner hit tied to persistence is materially stronger than an isolated hit.
- Execution evidence (strengthens confidence)
  - Validate whether the binary was likely run:
    - process creation artifacts (where available)
    - prefetch entries (if enabled)
    - user artifact references (LNKs, Jump Lists, Recent files)
  - Correlate timestamps: file creation/modification vs observed activity window.
- Surrounding artifact review (context building)
  - Check sibling files in the same directory (scripts, configs, dropped payloads).
  - Look for staging indicators (archives, renamed tools, log files, output artifacts).
  - Identify "operator habits" (naming patterns, directory structures, toolchains).
- Decision rule
  - Single hit = lead.
  - Hit + suspicious location + weak metadata = stronger lead.
  - Hit + persistence + execution evidence = evidence, high priority for escalation.